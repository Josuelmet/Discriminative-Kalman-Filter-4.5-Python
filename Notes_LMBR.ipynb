{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93602d8f-e252-4ebc-8475-55e7b3d53991",
   "metadata": {},
   "source": [
    "# LMBR Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f9b49-1598-438c-ab70-ebc19eb81c00",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt (LM) Algorithm\n",
    "\n",
    "### 1) Basic Gradient Descent:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x_{n+1}} &= \\mathbf{x_n} - \\lambda \\nabla f( \\mathbf{x_n} )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 2) Newton-Raphson (NR) method:\n",
    "\n",
    "2a) The solution for the stationary point of f(x):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla f( \\mathbf{x_{n+1}} ) &= \\nabla f( \\mathbf{x_n} ) + (\\mathbf{x_{n+1}} - \\mathbf{x_n})^T \\nabla^2 f( \\mathbf{x_n} ) +\n",
    "\\text{Higher order terms = 0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2b) Neglecting the higher-order terms, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x_{n+1}} &= \\mathbf{x_n} - ( \\nabla^2 f( \\mathbf{x_n} ) )^{-1} \\nabla f( \\mathbf{x_n} )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This method uses the inverse of the Hessian matrix $H = \\nabla^2 f( \\mathbf{x_n} )$ (i.e., the function's curvature) to reach the minimum point.\n",
    "\n",
    "Drawbacks of the NR method include:\n",
    "* Convergence depends strongly on the initial guess. Starting in a low-gradient (i.e., flat) region of the loss function will likely impede optimization.\n",
    "* The Hessian matrix must be positive-definite and not singular. Otherwise, the search direction may result in gradient ascent, or the algorithm stops functioning entirely.\n",
    "* Calculation of the Hessian matrix and its inverse is very time-consuming for large-dimension $\\mathbf{x}$ vectors.\n",
    "\n",
    "### 3) Levenberg-Marquardt (LM) Optimization:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x_{n+1}} &= \\mathbf{x_n} - ( \\nabla^2 f( \\mathbf{x_n} ) + \\lambda I )^{-1} \\nabla f( \\mathbf{x_n} )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This method combines basic gradient descent and NR optimization.\n",
    "* When $\\lambda \\rightarrow 0$, the LM method approaches the NR method.\n",
    "* When $\\lambda \\rightarrow \\infty$, the LM method approaches basic gradient descent with a small learning rate (step size) of $\\lambda^{-1}$.\n",
    "\n",
    "The gain, $\\lambda$, ensures that $\\lambda I + H$ is positive-definite (thus avoiding gradient ascent), even if $H$ is not.\n",
    "\n",
    "In each update step:\n",
    "* If loss decreases (i.e., gradient descent), then information from $H$ is helping, so $\\lambda$ is reduced (usually by a factor of 10).\n",
    "* If loss increases (i.e., gradient ascent), then information from $H$ is not helping. Thus, the step is rejected, $\\lambda$ is increased (usually by a factor of 10), and the step is recalculated in such fashion until loss decreases.\n",
    "\n",
    "For small optimization problems, LM optimization is faster than basic gradient descent and more stable than NR optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151eb1c-c989-4fc4-8895-2419c5a7ab31",
   "metadata": {},
   "source": [
    "## LM Optimization with Bayesian Regularization\n",
    "\n",
    "Source: \"Gauss-Newton Approximation to Bayesian Learning\" by Foresee and Hagan, 1997.\n",
    "\n",
    "## 1) Loss Function and Objective Function Parameters\n",
    "\n",
    "The objective is to minimize both $E_D$ and $E_W$, where $E_D$ is the sum of squared errors between network responses and training data, $D$, while $E_W$ is the sum of squares of network weights, $W$. Such regularization should produce accurate, generalized nonlinear regressions.\n",
    "\n",
    "Thus, the objective function is $F = \\beta E_D + \\alpha E_W $, where the initial values of $\\alpha$ and $\\beta$ are hyperparameters.\n",
    "* If $\\beta >> \\alpha$, then optimization will focus on minimizing errors.\n",
    "* If $\\beta << \\alpha$, then optimization will focus on minimizing weight sizes.\n",
    "\n",
    "## 2) Initialization\n",
    "\n",
    "The authors chose to use initial values of $\\alpha = 0$ and $\\beta = 1$, and to initialize weights via the Nguyen-Widrow method (1990).\n",
    "\n",
    "## 3) Training\n",
    "\n",
    "1) Take one step of LM optimization to minimize the objective function $F(W) = \\beta E_D + \\alpha E_W$. \n",
    "\n",
    "2) Compute the effective number of parameters $\\gamma = N - 2 \\alpha tr(H^{-1})$, where N is the total number of parameters in the network. \\\n",
    "   $\\gamma$ measures how many parameters in the network are effectively used in reducing the error function. \\\n",
    "   Make use of the Gauss-Newton approximation of $H$ available in the LM algorithm:\n",
    "   $H = \\nabla^2 F(W) \\approx 2 \\beta J^T J + 2 \\alpha I_N$, where $J$ is the Jacobian matrix of the training set errors.\n",
    "\n",
    "3) Compute new estimates for the objective function parameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha &= \\frac{\\gamma}{2 E_W(W)} \\\\\n",
    "\\beta &= \\frac{n - \\gamma}{2 E_D(W)}\n",
    "\\end{align}\n",
    "$$ \\\n",
    "where $n$ is the total number of training data. \\\n",
    "One alternate way to calculate $\\alpha$ and avoid iteration deficiency is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha &= \\frac{N}{2 E_W + tr(H^{-1})}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "4) Repeat steps 1 - 3 until convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
